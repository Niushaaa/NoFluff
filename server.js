const express = require('express');
const cors = require('cors');
require('dotenv').config();

const app = express();
const PORT = process.env.PORT || 3001;

// Middleware
app.use(cors());
app.use(express.json());

// Health check endpoint
app.get('/api/health', (req, res) => {
  res.json({ status: 'Backend is running!' });
});

// Download audio, extract transcript, and analyze highlights with real-time progress
app.get('/api/process/:videoId', async (req, res) => {
  try {
    const { videoId } = req.params;
    console.log(`Processing video: ${videoId}`);
    
    // Set up SSE headers for real-time progress updates
    res.writeHead(200, {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Headers': 'Cache-Control'
    });
    
    const sendProgress = (stage, progress, message) => {
      const data = JSON.stringify({ stage, progress, message });
      res.write(`data: ${data}\n\n`);
    };
    
    const start = Date.now();
    
    try {
      // Step 1: Download audio using yt-dlp (20% progress)
      sendProgress('downloading', 20, 'Downloading audio from YouTube...');
      const audioPath = await downloadAudio(videoId);
      console.log(`Audio downloaded: ${audioPath}`);
      
      // Step 2: Transcribe using OpenAI Whisper (60% progress)
      sendProgress('transcribing', 60, 'Transcribing audio with OpenAI Whisper...');
      const transcript = await transcribeAudio(audioPath, videoId);
      
      const transcriptEnd = Date.now();
      console.log(`Transcript generated in ${transcriptEnd - start}ms`);
      
      // Print transcript content to logs
      console.log('\n=== TRANSCRIPT CONTENT ===');
      console.log(`Total segments: ${transcript.segments.length}`);
      console.log(`Total duration: ${transcript.totalDuration || 'unknown'} seconds`);
      console.log('Segments:');
      transcript.segments.forEach((segment, index) => {
        const startTime = segment.start.toFixed(1);
        const endTime = (segment.start + segment.duration).toFixed(1);
        console.log(`[${index + 1}] ${startTime}s-${endTime}s: ${segment.text.trim()}`);
      });
      console.log('=== END TRANSCRIPT ===\n');
      
      // Step 3: Analyze highlights with AI (90% progress)
      sendProgress('analyzing', 90, 'AI analyzing transcript for best highlights...');
      const highlights = await analyzeHighlights(transcript);
      
      const end = Date.now();
      console.log(`Full processing completed in ${end - start}ms`);
      
      // Step 4: Clean up and send final result (100% progress)
      sendProgress('completing', 100, 'Processing complete! Preparing highlights...');
      
      const fs = require('fs');
      try {
        fs.unlinkSync(audioPath);
        console.log('Temporary audio file cleaned up');
      } catch (cleanupError) {
        console.warn('Could not clean up audio file:', cleanupError.message);
      }
      
      // Send final result
      const result = {
        transcript: {
          segments: transcript.segments,
          language: transcript.language || 'en',
          isAutoGenerated: false,
          totalSegments: transcript.segments.length,
          totalDuration: transcript.totalDuration
        },
        highlights: highlights,
        timing: {
          transcriptTime: transcriptEnd - start,
          totalTime: end - start
        }
      };
      
      res.write(`event: complete\ndata: ${JSON.stringify(result)}\n\n`);
      res.end();
      
    } catch (error) {
      console.error('Video processing error:', error);
      sendProgress('error', 0, `Error: ${error.message}`);
      res.write(`event: error\ndata: ${JSON.stringify({ error: error.message, videoId })}\n\n`);
      res.end();
    }
    
  } catch (error) {
    console.error('SSE setup error:', error);
    res.status(500).json({ 
      error: 'Failed to set up processing stream',
      message: error.message,
      videoId: req.params.videoId
    });
  }
});



// Helper function to download audio using yt-dlp
async function downloadAudio(videoId) {
  const { exec } = require('child_process');
  const { promisify } = require('util');
  const execAsync = promisify(exec);
  
  const outputPath = `./temp_audio_${videoId}.webm`;
  const videoUrl = `https://www.youtube.com/watch?v=${videoId}`;
  
  // Download best audio format (this is what worked in our tests)
  const command = `./yt-dlp -f "bestaudio" "${videoUrl}" --output "${outputPath}" --extractor-args "youtube:player_client=default"`;
  
  console.log('Downloading audio...');
  await execAsync(command);
  
  return outputPath;
}

// Helper function to transcribe audio using OpenAI Whisper
async function transcribeAudio(audioPath, videoId) {
  const { exec } = require('child_process');
  const { promisify } = require('util');
  const execAsync = promisify(exec);
  
  if (!process.env.OPENAI_API_KEY) {
    throw new Error('OPENAI_API_KEY not found in environment variables');
  }
  
  console.log('Transcribing audio with OpenAI Whisper...');
  
  // Retry logic for OpenAI API calls
  const maxRetries = 3;
  let lastError;
  
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      console.log(`Transcription attempt ${attempt}/${maxRetries}...`);
      return await attemptTranscription(audioPath);
    } catch (error) {
      lastError = error;
      console.error(`Attempt ${attempt} failed:`, error.message);
      
      // If this is a server error (502, 503, etc.), wait and retry
      if (error.message.includes('502') || error.message.includes('503') || error.message.includes('error code:')) {
        if (attempt < maxRetries) {
          const waitTime = attempt * 2000; // 2s, 4s, 6s
          console.log(`Waiting ${waitTime}ms before retry...`);
          await new Promise(resolve => setTimeout(resolve, waitTime));
          continue;
        }
      }
      
      // For other errors, don't retry
      throw error;
    }
  }
  
  throw lastError;
}

// Helper function to merge short segments into medium-length ones (5-15 seconds)
function mergeShortSegments(segments) {
  const TARGET_MIN_DURATION = 4; // Minimum 4 seconds per segment
  const merged = [];
  let currentSegment = null;
  
  for (const segment of segments) {
    if (!currentSegment) {
      // Start new segment
      currentSegment = {
        text: segment.text,
        start: segment.start,
        end: segment.end,
        duration: segment.duration
      };
    } else if (currentSegment.duration < TARGET_MIN_DURATION) {
      // Merge with current if too short
      currentSegment.text += ' ' + segment.text;
      currentSegment.end = segment.end;
      currentSegment.duration = currentSegment.end - currentSegment.start;
    } else {
      // Current segment is good length, save it and start new one
      merged.push({
        text: currentSegment.text,
        start: currentSegment.start,
        duration: currentSegment.duration
      });
      currentSegment = {
        text: segment.text,
        start: segment.start,
        end: segment.end,
        duration: segment.duration
      };
    }
  }
  
  // Don't forget the last segment
  if (currentSegment) {
    merged.push({
      text: currentSegment.text,
      start: currentSegment.start,
      duration: currentSegment.duration
    });
  }
  
  console.log(`Merged ${segments.length} segments into ${merged.length} medium-length segments`);
  return merged;
}

// Helper function for actual transcription attempt
async function attemptTranscription(audioPath) {
  const fs = require('fs');
  const FormData = require('form-data');
  
  // Check if audio file exists and has content
  try {
    const stats = fs.statSync(audioPath);
    console.log(`Audio file size: ${stats.size} bytes`);
    if (stats.size === 0) {
      throw new Error('Audio file is empty');
    }
    if (stats.size < 1000) {
      throw new Error(`Audio file too small: ${stats.size} bytes`);
    }
  } catch (error) {
    throw new Error(`Audio file issue: ${error.message}`);
  }
  
  // Use curl directly since fetch with FormData is having issues
  const { exec } = require('child_process');
  const { promisify } = require('util');
  const execAsync = promisify(exec);
  
  const tempResultFile = `./whisper_result_${Date.now()}.json`;
  
  // Create a temporary config file for curl to avoid exposing API key in process list
  const tempConfigFile = `./curl_config_${Date.now()}.txt`;
  const curlConfig = `
header = "Authorization: Bearer ${process.env.OPENAI_API_KEY}"
header = "Content-Type: multipart/form-data"
form = "file=@${audioPath}"
form = "model=whisper-1"
form = "response_format=verbose_json"
output = "${tempResultFile}"
silent
`;
  
  fs.writeFileSync(tempConfigFile, curlConfig);
  
  const curlCommand = `curl -X POST "https://api.openai.com/v1/audio/transcriptions" --config "${tempConfigFile}"`;
  
  console.log('Using secure curl for Whisper API call...');
  try {
    await execAsync(curlCommand);
  } finally {
    // Clean up config file immediately
    try { fs.unlinkSync(tempConfigFile); } catch {}
  }
  
  // Read the result
  const resultText = fs.readFileSync(tempResultFile, 'utf8');
  fs.unlinkSync(tempResultFile); // Clean up temp file
  
  // Check if the result contains an error
  let apiResult;
  try {
    apiResult = JSON.parse(resultText);
  } catch (parseError) {
    throw new Error(`Invalid JSON response: ${resultText.substring(0, 200)}...`);
  }
  
  // Check for API errors
  if (apiResult.error) {
    throw new Error(`OpenAI API error: ${JSON.stringify(apiResult.error)}`);
  }
  
  // Create a mock response object
  const response = {
    ok: true,
    json: () => apiResult,
    text: () => resultText
  };
  
  // Check if response looks like an error
  if (!response.ok) {
    const errorText = await response.text();
    console.error('OpenAI API error response:', errorText);
    throw new Error(`OpenAI API error (${response.status}): ${errorText}`);
  }
  
  let result;
  try {
    result = await response.json();
  } catch (parseError) {
    const responseText = await response.text();
    console.error('Failed to parse OpenAI response as JSON:', parseError.message);
    console.error('Raw response was:', responseText);
    throw new Error(`OpenAI API returned invalid JSON. Response: ${responseText.substring(0, 200)}...`);
  }
  
  // Log successful response for debugging
  console.log('OpenAI Whisper API response received successfully');
  
  if (result.error) {
    throw new Error(`OpenAI API error: ${result.error.message}`);
  }
  
  console.log(`Whisper transcribed successfully`);
  console.log(`Duration: ${result.usage?.seconds || 'unknown'} seconds`);
  
  // Parse the response - verbose_json gives us segments with timestamps
  if (result.segments && result.segments.length > 0) {
    // Convert segments to our format first
    const rawSegments = result.segments.map(seg => ({
      text: seg.text.trim(),
      start: seg.start,
      end: seg.end,
      duration: seg.end - seg.start
    }));
    
    // Merge short segments into medium-length ones (5-15 seconds)
    const mergedSegments = mergeShortSegments(rawSegments);
    
    return {
      segments: mergedSegments,
      language: result.language,
      totalDuration: result.usage?.seconds || 120 // Real duration from Whisper
    };
  } else {
    // Fallback: split text into segments (when verbose_json doesn't return segments)
    const text = result.text || '';
    const estimatedDuration = result.usage?.seconds || 120;
    const words = text.split(' ');
    const segmentSize = Math.max(10, Math.floor(words.length / 20)); // ~20 segments max
    
    const segments = [];
    for (let i = 0; i < words.length; i += segmentSize) {
      const segmentWords = words.slice(i, i + segmentSize);
      const segmentText = segmentWords.join(' ');
      const startTime = (i / words.length) * estimatedDuration;
      const endTime = Math.min(((i + segmentSize) / words.length) * estimatedDuration, estimatedDuration);
      
      segments.push({
        text: segmentText,
        start: startTime,
        duration: endTime - startTime
      });
    }
    
    return {
      segments,
      language: result.language || 'en',
      totalDuration: estimatedDuration
    };
  }
}

// Helper function to analyze highlights 
async function analyzeHighlights(transcript) {
  if (!transcript || !transcript.segments) {
    throw new Error('Invalid transcript data');
  }

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [{
          role: 'user',
          content: `You are a professional video highlight creator.
Your job is to analyze a pre-segmented transcript and extract the most important, interesting, and narrative-defining moments.

The transcript is provided in this format: [index] start_time-end_time: text

Each line is a segment.
Use these segments exactly as given.

TASK:

Follow these steps precisely:

1. Create Chunks
	‚Ä¢	Group the transcript into chunks of 10 consecutive segments.
	‚Ä¢	Keep all segments in original order.
	‚Ä¢	Each chunk represents ~30 seconds of video.

2. Summarize Each Chunk
For every chunk, produce:
	‚Ä¢	A one-sentence core message
	‚Ä¢	A 1 or 2 sentence explanation of why that chunk matters

Be concise, but capture meaning and relevance.

3. Find the Video‚Äôs Narrative

Using all core messages from the chunks:
	‚Ä¢	Identify the video‚Äôs central theme, narrative arc, and purpose
	‚Ä¢	Write a 3‚Äì4 sentence explanation summarizing the big picture

This is the message the highlights must preserve.

4. Select Highlight Segments (TOTAL = EXACTLY 20 SECONDS)

You must:
	‚Ä¢	Choose individual segments (from the original transcript)
	‚Ä¢	Choose at most 8 segments
	‚Ä¢	Select segments that best:
	‚Ä¢	Express the video‚Äôs core message
	‚Ä¢	Preserve the narrative arc
	‚Ä¢	Are interesting, emotional, insightful, or impactful


Transcript with timestamps:
${transcript.segments.map(seg => `[${seg.start}s-${(seg.start + seg.duration).toFixed(1)}s]: ${seg.text}`).join('\n')}

CRITICAL REQUIREMENTS:
Return a JSON array of highlight segments. Each segment should be:
{
  "id": "highlight_1", 
  "name": "Short descriptive title (2-4 words)",
  "startTime": start_time_in_seconds,
  "endTime": end_time_in_seconds,
  "reason": "brief explanation why this is important"
}

- Return only valid JSON array, no other text or markdown`
        }],
        temperature: 0.3,
        max_tokens: 1000
      }),
    });

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.status}`);
    }

    const result = await response.json();
    const content = result.choices[0]?.message?.content;
    
    if (!content) {
      throw new Error('No response from OpenAI');
    }

    // Parse the JSON response (handle markdown code blocks)
    let cleanContent = content.trim();
    if (cleanContent.startsWith('```json')) {
      cleanContent = cleanContent.replace(/^```json\s*/, '').replace(/\s*```$/, '');
    } else if (cleanContent.startsWith('```')) {
      cleanContent = cleanContent.replace(/^```\s*/, '').replace(/\s*```$/, '');
    }
    
    // Log the raw AI response for debugging
    console.log('Raw AI response:', cleanContent);
    
    let highlights;
    try {
      highlights = JSON.parse(cleanContent);
    } catch (parseError) {
      console.error('JSON parse error:', parseError.message);
      console.error('Problematic content:', cleanContent);
      
      // Try to fix common JSON issues
      let fixedContent = cleanContent
        .replace(/\n/g, '\\n')  // Escape newlines
        .replace(/\r/g, '\\r')  // Escape carriage returns  
        .replace(/\t/g, '\\t')  // Escape tabs
        .replace(/"/g, '\\"')   // Escape quotes
        .replace(/\\"/g, '"')   // Un-escape the outer quotes
        .replace(/^"/, '')      // Remove leading quote if any
        .replace(/"$/, '');     // Remove trailing quote if any
      
      try {
        highlights = JSON.parse(fixedContent);
        console.log('Fixed JSON parsing succeeded');
      } catch (secondError) {
        console.error('Even after fixing, JSON parse failed:', secondError.message);
        throw new Error('AI returned invalid JSON format');
      }
    }
    console.log(`AI identified ${highlights.length} highlight segments`);
    
    // Print AI-selected highlights to logs
    console.log('\n=== AI SELECTED HIGHLIGHTS ===');
    let totalDuration = 0;
    highlights.forEach((highlight, index) => {
      const duration = highlight.endTime - highlight.startTime;
      totalDuration += duration;
      console.log(`[${index + 1}] ${highlight.name || highlight.id}`);
      console.log(`    Time: ${highlight.startTime.toFixed(1)}s - ${highlight.endTime.toFixed(1)}s (${duration.toFixed(1)}s)`);
      console.log(`    Reason: ${highlight.reason}`);
      console.log('');
    });
    console.log(`Total highlight duration: ${totalDuration.toFixed(1)} seconds`);
    console.log('=== END AI HIGHLIGHTS ===\n');

    return highlights;
}


// Start server
app.listen(PORT, () => {
  console.log(`üöÄ NoFluff Backend running on http://localhost:${PORT}`);
  console.log(`üé¨ Video Processing API: http://localhost:${PORT}/api/process/:videoId`);
  
  if (process.env.OPENAI_API_KEY) {
    console.log(`‚úÖ OpenAI API key loaded - Whisper + GPT-4o-mini ready`);
  } else {
    console.log(`‚ö†Ô∏è  OpenAI API key not found - check .env file`);
  }
});

module.exports = app;