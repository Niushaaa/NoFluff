const express = require('express');
const cors = require('cors');
require('dotenv').config();

const app = express();
const PORT = process.env.PORT || 3001;

// Middleware
app.use(cors());
app.use(express.json());

// Health check endpoint
app.get('/api/health', (req, res) => {
  res.json({ status: 'Backend is running!' });
});

// Download audio and extract transcript using yt-dlp + OpenAI Whisper
app.get('/api/transcript/:videoId', async (req, res) => {
  try {
    const { videoId } = req.params;
    console.log(`Processing video: ${videoId}`);
    
    const start = Date.now();
    
    // Step 1: Download audio using yt-dlp
    const audioPath = await downloadAudio(videoId);
    console.log(`Audio downloaded: ${audioPath}`);
    
    // Step 2: Transcribe using OpenAI Whisper (requires API key)
    const transcript = await transcribeAudio(audioPath, videoId);
    
    const end = Date.now();
    console.log(`Transcript generated in ${end - start}ms`);
    
    // Print transcript content to logs
    console.log('\n=== TRANSCRIPT CONTENT ===');
    console.log(`Total segments: ${transcript.segments.length}`);
    console.log(`Total duration: ${transcript.totalDuration || 'unknown'} seconds`);
    console.log('Segments:');
    transcript.segments.forEach((segment, index) => {
      const startTime = segment.start.toFixed(1);
      const endTime = (segment.start + segment.duration).toFixed(1);
      console.log(`[${index + 1}] ${startTime}s-${endTime}s: ${segment.text.trim()}`);
    });
    console.log('=== END TRANSCRIPT ===\n');
    
    // Step 3: Clean up audio file
    const fs = require('fs');
    try {
      fs.unlinkSync(audioPath);
      console.log('Temporary audio file cleaned up');
    } catch (cleanupError) {
      console.warn('Could not clean up audio file:', cleanupError.message);
    }
    
    res.json({
      segments: transcript.segments,
      language: transcript.language || 'en',
      isAutoGenerated: false, // Whisper transcription, not auto-generated
      totalSegments: transcript.segments.length,
      fetchTime: end - start,
      totalDuration: transcript.totalDuration // Real duration from Whisper
    });
    
  } catch (error) {
    console.error('Transcript processing error:', error);
    res.status(500).json({ 
      error: 'Failed to process video transcript',
      message: error.message,
      videoId: req.params.videoId
    });
  }
});

// YouTube video metadata endpoint (placeholder)
app.get('/api/video/:videoId', async (req, res) => {
  try {
    const { videoId } = req.params;
    
    // Basic metadata without external dependencies
    res.json({
      id: videoId,
      title: `Video ${videoId}`,
      duration: 0, // Will be updated with real duration later
      url: `https://youtube.com/watch?v=${videoId}`,
      thumbnailUrl: `https://img.youtube.com/vi/${videoId}/maxresdefault.jpg`
    });
    
  } catch (error) {
    console.error('Video metadata error:', error);
    res.status(500).json({ 
      error: 'Failed to fetch video metadata',
      message: error.message
    });
  }
});

// Helper function to download audio using yt-dlp
async function downloadAudio(videoId) {
  const { exec } = require('child_process');
  const { promisify } = require('util');
  const execAsync = promisify(exec);
  
  const outputPath = `./temp_audio_${videoId}.webm`;
  const videoUrl = `https://www.youtube.com/watch?v=${videoId}`;
  
  // Download best audio format (this is what worked in our tests)
  const command = `./yt-dlp -f "bestaudio" "${videoUrl}" --output "${outputPath}" --extractor-args "youtube:player_client=default"`;
  
  console.log('Downloading audio...');
  await execAsync(command);
  
  return outputPath;
}

// Helper function to transcribe audio using OpenAI Whisper
async function transcribeAudio(audioPath, videoId) {
  const { exec } = require('child_process');
  const { promisify } = require('util');
  const execAsync = promisify(exec);
  
  if (!process.env.OPENAI_API_KEY) {
    throw new Error('OPENAI_API_KEY not found in environment variables');
  }
  
  console.log('Transcribing audio with OpenAI Whisper...');
  
  // Use curl for reliable multipart upload (our form-data library has issues)
  const curlCommand = `curl -X POST "https://api.openai.com/v1/audio/transcriptions" \
    -H "Authorization: Bearer ${process.env.OPENAI_API_KEY}" \
    -F "file=@${audioPath}" \
    -F "model=whisper-1" \
    -F "response_format=verbose_json"`;
  
  const { stdout } = await execAsync(curlCommand);
  const result = JSON.parse(stdout);
  
  if (result.error) {
    throw new Error(`OpenAI API error: ${result.error.message}`);
  }
  
  console.log(`Whisper transcribed successfully`);
  console.log(`Duration: ${result.usage?.seconds || 'unknown'} seconds`);
  
  // Parse the response - verbose_json gives us segments with timestamps
  if (result.segments && result.segments.length > 0) {
    return {
      segments: result.segments.map(seg => ({
        text: seg.text.trim(),
        start: seg.start,
        duration: seg.end - seg.start
      })),
      language: result.language,
      totalDuration: result.usage?.seconds || 120 // Real duration from Whisper
    };
  } else {
    // Fallback: split text into segments (when verbose_json doesn't return segments)
    const text = result.text || '';
    const estimatedDuration = result.usage?.seconds || 120;
    const words = text.split(' ');
    const segmentSize = Math.max(10, Math.floor(words.length / 20)); // ~20 segments max
    
    const segments = [];
    for (let i = 0; i < words.length; i += segmentSize) {
      const segmentWords = words.slice(i, i + segmentSize);
      const segmentText = segmentWords.join(' ');
      const startTime = (i / words.length) * estimatedDuration;
      const endTime = Math.min(((i + segmentSize) / words.length) * estimatedDuration, estimatedDuration);
      
      segments.push({
        text: segmentText,
        start: startTime,
        duration: endTime - startTime
      });
    }
    
    return {
      segments,
      language: result.language || 'en',
      totalDuration: estimatedDuration
    };
  }
}

// AI highlight analysis endpoint
app.post('/api/analyze-highlights', async (req, res) => {
  try {
    const { transcript } = req.body;
    
    if (!transcript || !transcript.segments) {
      return res.status(400).json({ error: 'Invalid transcript data' });
    }

    console.log('Analyzing transcript with OpenAI GPT-4o-mini...');

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [{
          role: 'user',
          content: `You are a professional video highlight creator.
Your job is to analyze a pre-segmented transcript and extract the most important, interesting, and narrative-defining moments.

The transcript is provided in this format: [index] start_time-end_time: text

Each line is a segment.
Use these segments exactly as given.

TASK:

Follow these steps precisely:

1. Create Chunks
	‚Ä¢	Group the transcript into chunks of 10 consecutive segments.
	‚Ä¢	Keep all segments in original order.
	‚Ä¢	Each chunk represents ~30 seconds of video.

2. Summarize Each Chunk
For every chunk, produce:
	‚Ä¢	A one-sentence core message
	‚Ä¢	A 1 or 2 sentence explanation of why that chunk matters

Be concise, but capture meaning and relevance.

3. Find the Video‚Äôs Narrative

Using all core messages from the chunks:
	‚Ä¢	Identify the video‚Äôs central theme, narrative arc, and purpose
	‚Ä¢	Write a 3‚Äì4 sentence explanation summarizing the big picture

This is the message the highlights must preserve.

4. Select Highlight Segments (TOTAL = EXACTLY 20 SECONDS)

You must:
	‚Ä¢	Choose individual segments (from the original transcript)
	‚Ä¢	Choose at most 8 segments
	‚Ä¢	Select segments that best:
	‚Ä¢	Express the video‚Äôs core message
	‚Ä¢	Preserve the narrative arc
	‚Ä¢	Are interesting, emotional, insightful, or impactful


Transcript with timestamps:
${transcript.segments.map(seg => `[${seg.start}s-${(seg.start + seg.duration).toFixed(1)}s]: ${seg.text}`).join('\n')}

CRITICAL REQUIREMENTS:
Return a JSON array of highlight segments. Each segment should be:
{
  "id": "highlight_1", 
  "name": "Short descriptive title (2-4 words)",
  "startTime": start_time_in_seconds,
  "endTime": end_time_in_seconds,
  "reason": "brief explanation why this is important"
}

- Return only valid JSON array, no other text or markdown`
        }],
        temperature: 0.3,
        max_tokens: 1000
      }),
    });

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.status}`);
    }

    const result = await response.json();
    const content = result.choices[0]?.message?.content;
    
    if (!content) {
      throw new Error('No response from OpenAI');
    }

    // Parse the JSON response (handle markdown code blocks)
    let cleanContent = content.trim();
    if (cleanContent.startsWith('```json')) {
      cleanContent = cleanContent.replace(/^```json\s*/, '').replace(/\s*```$/, '');
    } else if (cleanContent.startsWith('```')) {
      cleanContent = cleanContent.replace(/^```\s*/, '').replace(/\s*```$/, '');
    }
    
    // Log the raw AI response for debugging
    console.log('Raw AI response:', cleanContent);
    
    let highlights;
    try {
      highlights = JSON.parse(cleanContent);
    } catch (parseError) {
      console.error('JSON parse error:', parseError.message);
      console.error('Problematic content:', cleanContent);
      
      // Try to fix common JSON issues
      let fixedContent = cleanContent
        .replace(/\n/g, '\\n')  // Escape newlines
        .replace(/\r/g, '\\r')  // Escape carriage returns  
        .replace(/\t/g, '\\t')  // Escape tabs
        .replace(/"/g, '\\"')   // Escape quotes
        .replace(/\\"/g, '"')   // Un-escape the outer quotes
        .replace(/^"/, '')      // Remove leading quote if any
        .replace(/"$/, '');     // Remove trailing quote if any
      
      try {
        highlights = JSON.parse(fixedContent);
        console.log('Fixed JSON parsing succeeded');
      } catch (secondError) {
        console.error('Even after fixing, JSON parse failed:', secondError.message);
        throw new Error('AI returned invalid JSON format');
      }
    }
    console.log(`AI identified ${highlights.length} highlight segments`);
    
    // Print AI-selected highlights to logs
    console.log('\n=== AI SELECTED HIGHLIGHTS ===');
    let totalDuration = 0;
    highlights.forEach((highlight, index) => {
      const duration = highlight.endTime - highlight.startTime;
      totalDuration += duration;
      console.log(`[${index + 1}] ${highlight.name || highlight.id}`);
      console.log(`    Time: ${highlight.startTime.toFixed(1)}s - ${highlight.endTime.toFixed(1)}s (${duration.toFixed(1)}s)`);
      console.log(`    Reason: ${highlight.reason}`);
      console.log('');
    });
    console.log(`Total highlight duration: ${totalDuration.toFixed(1)} seconds`);
    console.log('=== END AI HIGHLIGHTS ===\n');

    res.json({ highlights });

  } catch (error) {
    console.error('AI analysis error:', error);
    res.status(500).json({ 
      error: 'Failed to analyze highlights',
      message: error.message
    });
  }
});

// Start server
app.listen(PORT, () => {
  console.log(`üöÄ NoFluff Backend running on http://localhost:${PORT}`);
  console.log(`üìù Transcript API: http://localhost:${PORT}/api/transcript/:videoId`);
  console.log(`üé• Video API: http://localhost:${PORT}/api/video/:videoId`);
  console.log(`ü§ñ AI Analysis API: http://localhost:${PORT}/api/analyze-highlights`);
  
  if (process.env.OPENAI_API_KEY) {
    console.log(`‚úÖ OpenAI API key loaded - Whisper + GPT-4o-mini ready`);
  } else {
    console.log(`‚ö†Ô∏è  OpenAI API key not found - check .env file`);
  }
});

module.exports = app;